# 反爬虫

# 添加headers，header为请求头部信息  

# <https://www.zhuhu.com/explore>  

# 最基础也需要填写一个User_Agent 

## 1.伪造浏览器

import requests
header = {'User_Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'}
response = requests.get('https://www.zhihu.com/explore',headers = header)
print(response.text)

```
<html>
<head><title>400 Bad Request</title></head>
<body bgcolor="white">
<center><h1>400 Bad Request</h1></center>
<hr><center>openresty</center>
</body>
</html>
```

本机浏览器：{'User_Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'}

 # response属性

allow_redirects=False不允许跳转（返回301）

status_code **

headers**

cookies**

url**（网址）

history（历史）

‘http://www.jianshu.com’

例如：

import requests
header = {'User_Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'}
response = requests.get('http://www.jianshu.com',headers = header,allow_redirects=True)
print(response.text)
print(response.url)
print(response.history)
    

```
<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
<html>
<head><title>403 Forbidden</title></head>
<body bgcolor="white">
<h1>403 Forbidden</h1>
<p>You don't have permission to access the URL on this server. Sorry for the inconvenience.<br/>
Please report this message and include the following information to us.<br/>
Thank you very much!</p>
<table>
<tr>
<td>URL:</td>
<td>https://www.jianshu.com/</td>
</tr>
<tr>
<td>Server:</td>
<td>zurich</td>
</tr>
<tr>
<td>Date:</td>
<td>2018/06/10 11:45:10</td>
</tr>
</table>
<hr/>Powered by Tengine</body>
</html>

https://www.jianshu.com/
[<Response [301]>]
```



# 作业：

# 定义一个函数

# 抓取http://www.17k.com的随意两个页面，存入本地

def  

​    a=[路由]

foriinrange(2)

​    url = a[i]

  res=requests(url,hearders= )



